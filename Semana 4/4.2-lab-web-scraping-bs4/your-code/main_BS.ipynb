{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-Scraping-Lab\" data-toc-modified-id=\"Web-Scraping-Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Web Scraping Lab</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Useful-Resources\" data-toc-modified-id=\"Useful-Resources-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Useful Resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-of-all,-gathering-our-tools.\" data-toc-modified-id=\"First-of-all,-gathering-our-tools.-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span>First of all, gathering our tools.</a></span></li><li><span><a href=\"#Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:\" data-toc-modified-id=\"Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:-1.0.1.2\"><span class=\"toc-item-num\">1.0.1.2&nbsp;&nbsp;</span>Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:</a></span></li><li><span><a href=\"#Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.\" data-toc-modified-id=\"Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.-1.0.1.3\"><span class=\"toc-item-num\">1.0.1.3&nbsp;&nbsp;</span>Display the names of the trending developers retrieved in the previous step.</a></span></li><li><span><a href=\"#Challenge-2---Display-the-trending-Python-repositories-in-GitHub\" data-toc-modified-id=\"Challenge-2---Display-the-trending-Python-repositories-in-GitHub-1.0.1.4\"><span class=\"toc-item-num\">1.0.1.4&nbsp;&nbsp;</span>Challenge 2 - Display the trending Python repositories in GitHub</a></span></li><li><span><a href=\"#Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page\" data-toc-modified-id=\"Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page-1.0.1.5\"><span class=\"toc-item-num\">1.0.1.5&nbsp;&nbsp;</span>Challenge 3 - Display all the image links from Walt Disney wikipedia page</a></span></li><li><span><a href=\"#Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.\" data-toc-modified-id=\"Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.-1.0.1.6\"><span class=\"toc-item-num\">1.0.1.6&nbsp;&nbsp;</span>Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python.</a></span></li><li><span><a href=\"#Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point\" data-toc-modified-id=\"Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point-1.0.1.7\"><span class=\"toc-item-num\">1.0.1.7&nbsp;&nbsp;</span>Challenge 5 - Number of Titles that have changed in the United States Code since its last release point</a></span></li><li><span><a href=\"#Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names\" data-toc-modified-id=\"Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names-1.0.1.8\"><span class=\"toc-item-num\">1.0.1.8&nbsp;&nbsp;</span>Challenge 6 - A Python list with the top ten FBI's Most Wanted names</a></span></li><li><span><a href=\"#Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org\" data-toc-modified-id=\"Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org-1.0.1.9\"><span class=\"toc-item-num\">1.0.1.9&nbsp;&nbsp;</span>Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org</a></span></li><li><span><a href=\"#Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk\" data-toc-modified-id=\"Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk-1.0.1.10\"><span class=\"toc-item-num\">1.0.1.10&nbsp;&nbsp;</span>Challenge 8 - A list with the different kind of datasets available in data.gov.uk</a></span></li><li><span><a href=\"#Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe\" data-toc-modified-id=\"Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe-1.0.1.11\"><span class=\"toc-item-num\">1.0.1.11&nbsp;&nbsp;</span>Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe</a></span></li></ul></li><li><span><a href=\"#Stepping-up-the-game\" data-toc-modified-id=\"Stepping-up-the-game-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Stepping up the game</a></span><ul class=\"toc-item\"><li><span><a href=\"#Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span>Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe-1.0.2.2\"><span class=\"toc-item-num\">1.0.2.2&nbsp;&nbsp;</span>Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.-1.0.2.3\"><span class=\"toc-item-num\">1.0.2.3&nbsp;&nbsp;</span>Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe.</a></span></li><li><span><a href=\"#Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.\" data-toc-modified-id=\"Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.-1.0.2.4\"><span class=\"toc-item-num\">1.0.2.4&nbsp;&nbsp;</span>Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city.</a></span></li><li><span><a href=\"#Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.-1.0.2.5\"><span class=\"toc-item-num\">1.0.2.5&nbsp;&nbsp;</span>Challenge 14 - Book name,price and stock availability as a pandas dataframe.</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some web scraping exercises to practice your scraping skills using `requests` and `Beautiful Soup`.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the [response status code](https://http.cat/) for each request to ensure you have obtained the intended content.\n",
    "- Look at the HTML code in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit each url and take a look at its source through Chrome DevTools. You'll need to identify the html tags, special class names etc. used for the html content you are expected to extract.\n",
    "- Check out the css selectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide) documentation \n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First of all, gathering our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Again, please remember to limit your output before submission so that your code doesn't get lost in the output.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_html=requests.get(url).text\n",
    "soup = BeautifulSoup(github_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below (with different names):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Alex Gaynor', 'alex'),\n",
       " ('Christian Muehlhaeuser', 'muesli'),\n",
       " ('Gwendal Roué', 'groue'),\n",
       " ('Liam Galvin', 'liamg'),\n",
       " ('Tom Payne', 'twpayne'),\n",
       " ('Casey Rodarmor', 'casey'),\n",
       " ('Jared Palmer', 'jaredpalmer'),\n",
       " ('Bram Kragten', 'bramkragten'),\n",
       " ('Nico Schlömer', 'nschloe'),\n",
       " ('Olivier Halligon', 'AliSoftware'),\n",
       " ('Chris Caron', 'caronc'),\n",
       " ('Felix Angelov', 'felangel'),\n",
       " ('MichaIng', 'mattlewis92'),\n",
       " ('Matt Lewis', 'stefanprodan'),\n",
       " ('Stefan Prodan', 'alecthomas'),\n",
       " ('Alec Thomas', 'Rob--W'),\n",
       " ('Rob Wu', 'mgmeyers'),\n",
       " ('Matthew Meyers', 'jacogr'),\n",
       " ('Jaco', 'yairm210'),\n",
       " ('Yair Morgenstern', 'LekoArts'),\n",
       " ('Lennart', 'zoontek'),\n",
       " ('Mathieu Acthernoene', 'tkf'),\n",
       " ('Takafumi Arakaki', 'sokra'),\n",
       " ('Tobias Koppers', 'kubkon')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract():\n",
    "    nombres = soup.select('h1.h3 a[href]')\n",
    "    alias = soup.select('p.f4 a[href]')\n",
    "    return [(nombre.text.strip(), alias.text.strip()) for (nombre,alias) in zip(nombres,alias)]\n",
    "extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Challenge 2 - Display the trending Python repositories in GitHub\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'\n",
    "datos2 = requests.get(f\"{url2}\").text\n",
    "soup2 = BeautifulSoup(datos2, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YT-Spammer-Purge', 'black', 'chia-blockchain', 'awesome-python', 'plane-notify', 'PatrikZeros-CSGO-Sound-Fix', 'netbox', 'pycord', 'openpilot', 'Tidal-Media-Downloader', 'pip-tools', 'textual', 'Awesome-Python-Scripts', 'wskey', 'explainerdashboard', 'ailab', 'optuna', 'rich-cli', 'river', 'mdt', 'jax', 'content', 'cookiecutter', 'ScoutSuite', 'yt-dlp']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def nombrerepo():\n",
    "    repo = soup2.select('h1.h3.lh-condensed a[href]')\n",
    "    return [rep.text.strip() for rep in repo]\n",
    "lista_nombres = nombrerepo()\n",
    "nombre = [re.findall(r\"(\\s(\\d|\\w).*$)\", e)for e in lista_nombres]\n",
    "nombre_final = [x[0].strip() for a in nombre for x in a]\n",
    "print(nombre_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 3 - Display all the image links from Walt Disney wikipedia page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "disney = requests.get(f\"{url3}\").text\n",
    "soup3 = BeautifulSoup(disney, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pillaenlaces():\n",
    "    datasoup = soup3.select('img')\n",
    "    return [enlace for enlace in datasoup]\n",
    "todos_enlaces = pillaenlaces()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "atributos = [a.attrs for a in todos_enlaces]\n",
    "enlaces = [\"https\" + enlace['src'] for enlace in atributos]\n",
    "display(enlaces[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 ='https://en.wikipedia.org/wiki/Python' \n",
    "python = requests.get(f\"{url4}\").text\n",
    "soup4 = BeautifulSoup(python, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://en.wiktionary.org/wiki/Python\n"
     ]
    }
   ],
   "source": [
    "tags = soup4('a')\n",
    "enlaces = [tag.get('href') for tag in tags]\n",
    "limpio = [e for e in enlaces[1:]]\n",
    "limpio2 = [h for h in limpio if \"https\" in h]\n",
    "print(limpio2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 5 - Number of Titles that have changed in the United States Code since its last release point "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n\\n          Title 1 - General Provisions ٭\\n',\n",
       " '\\n\\n          Title 2 - The Congress\\n\\n        ',\n",
       " '\\n\\n          Title 5 - Government Organization and Employees ٭\\n',\n",
       " '\\n\\n          Title 6 - Domestic Security\\n\\n        ',\n",
       " '\\n\\n          Title 7 - Agriculture\\n\\n        ',\n",
       " '\\n\\n          Title 12 - Banks and Banking\\n\\n        ',\n",
       " '\\n\\n          Title 15 - Commerce and Trade\\n\\n        ',\n",
       " '\\n\\n          Title 16 - Conservation\\n\\n        ',\n",
       " '\\n\\n          Title 19 - Customs Duties\\n\\n        ',\n",
       " '\\n\\n          Title 23 - Highways ٭\\n',\n",
       " '\\n\\n          Title 25 - Indians\\n\\n        ',\n",
       " '\\n\\n          Title 26 - Internal Revenue Code\\n\\n        ',\n",
       " '\\n\\n          Title 29 - Labor\\n\\n        ',\n",
       " '\\n\\n          Title 30 - Mineral Lands and Mining\\n\\n        ',\n",
       " '\\n\\n          Title 33 - Navigation and Navigable Waters\\n\\n        ',\n",
       " '\\n\\n          Title 40 - Public Buildings, Property, and Works ٭\\n',\n",
       " '\\n\\n          Title 41 - Public Contracts ٭\\n',\n",
       " '\\n\\n          Title 42 - The Public Health and Welfare\\n\\n        ',\n",
       " '\\n\\n          Title 43 - Public Lands\\n\\n        ',\n",
       " '\\n\\n          Title 45 - Railroads\\n\\n        ',\n",
       " '\\n\\n          Title 46 - Shipping ٭\\n',\n",
       " '\\n\\n          Title 47 - Telecommunications\\n\\n        ',\n",
       " '\\n\\n          Title 49 - Transportation ٭\\n',\n",
       " '\\n\\n          Title 54 - National Park Service and Related Programs ٭\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url6 = 'http://uscode.house.gov/download/download.shtml'\n",
    "eeuu = requests.get(f\"{url6}\").text\n",
    "soup6 = BeautifulSoup(eeuu, 'html.parser')\n",
    "libros_ = soup6.findAll(\"div\", {\"class\": \"usctitlechanged\"})\n",
    "libros = [libro.text for libro in libros_]\n",
    "libros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Title 1 - General Provisions'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libros[0].lstrip(\"\\n\\n          \").rstrip(\"٭\\n\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Title 1 - General Provisions',\n",
       " 'Title 2 - The Congress',\n",
       " 'Title 5 - Government Organization and Employees',\n",
       " 'Title 6 - Domestic Security',\n",
       " 'Title 7 - Agriculture',\n",
       " 'Title 12 - Banks and Banking',\n",
       " 'Title 15 - Commerce and Trade',\n",
       " 'Title 16 - Conservation',\n",
       " 'Title 19 - Customs Duties',\n",
       " 'Title 23 - Highways',\n",
       " 'Title 25 - Indians',\n",
       " 'Title 26 - Internal Revenue Code',\n",
       " 'Title 29 - Labor',\n",
       " 'Title 30 - Mineral Lands and Mining',\n",
       " 'Title 33 - Navigation and Navigable Waters',\n",
       " 'Title 40 - Public Buildings, Property, and Works',\n",
       " 'Title 41 - Public Contracts',\n",
       " 'Title 42 - The Public Health and Welfare',\n",
       " 'Title 43 - Public Lands',\n",
       " 'Title 45 - Railroads',\n",
       " 'Title 46 - Shipping',\n",
       " 'Title 47 - Telecommunications',\n",
       " 'Title 49 - Transportation',\n",
       " 'Title 54 - National Park Service and Related Programs']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libros_limpio = [libro.lstrip(\"\\n\\n          \").rstrip(\"٭\\n\").strip() for libro in libros]\n",
    "libros_limpio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 6 - A Python list with the top ten FBI's Most Wanted names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "wanted = requests.get(f\"{url7}\").text\n",
    "soup7 = BeautifulSoup(wanted, 'html.parser')\n",
    "wantedtag = soup7.find_all('h3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<h3 class=\"title\">\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/rafael-caro-quintero\">RAFAEL CARO-QUINTERO</a>\n",
       " </h3>,\n",
       " <h3 class=\"title\">\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/yulan-adonay-archaga-carias\">YULAN ADONAY ARCHAGA CARIAS</a>\n",
       " </h3>,\n",
       " <h3 class=\"title\">\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/eugene-palmer\">EUGENE PALMER</a>\n",
       " </h3>,\n",
       " <h3 class=\"title\">\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/bhadreshkumar-chetanbhai-patel\">BHADRESHKUMAR CHETANBHAI PATEL</a>\n",
       " </h3>,\n",
       " <h3 class=\"title\">\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/alejandro-castillo\">ALEJANDRO ROSALES CASTILLO</a>\n",
       " </h3>,\n",
       " <h3 class=\"title\">\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/arnoldo-jimenez\">ARNOLDO JIMENEZ</a>\n",
       " </h3>,\n",
       " <h3 class=\"title\">\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/jason-derek-brown\">JASON DEREK BROWN</a>\n",
       " </h3>,\n",
       " <h3 class=\"title\">\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/alexis-flores\">ALEXIS FLORES</a>\n",
       " </h3>,\n",
       " <h3 class=\"title\">\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/jose-rodolfo-villarreal-hernandez\">JOSE RODOLFO VILLARREAL-HERNANDEZ</a>\n",
       " </h3>,\n",
       " <h3 class=\"title\">\n",
       " <a href=\"https://www.fbi.gov/wanted/topten/octaviano-juarez-corro\">OCTAVIANO JUAREZ-CORRO</a>\n",
       " </h3>,\n",
       " <h3>federal bureau of investigation</h3>,\n",
       " <h3>\n",
       " FBI.gov Contact Center\n",
       " </h3>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wantedtag #we can see that the 2 last elements aren't registers of most wanted criminals. So we are going to exclude it from our list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_wanted = []\n",
    "for w in range(len(wantedtag)-2): #I substract 2 from the length to make sure we exclude the last 2 elements that have no atribute href, and would raise an error. \n",
    "    most_wanted.append((wantedtag[w].find_all(\"a\")[0]).get_text('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I substract 2 from the length to make sure we exclude the last 2 elements that have no atribute href, and would raise an error. \n",
    "\n",
    "most_wanted = [(wantedtag[w].find_all(\"a\")[0]).get_text('href') for w in range(len(wantedtag)-2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAFAEL CARO-QUINTERO',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'EUGENE PALMER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'OCTAVIANO JUAREZ-CORRO']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_wanted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAFAEL CARO-QUINTERO',\n",
       " 'YULAN ADONAY ARCHAGA CARIAS',\n",
       " 'EUGENE PALMER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'JOSE RODOLFO VILLARREAL-HERNANDEZ',\n",
       " 'OCTAVIANO JUAREZ-CORRO']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another solution\n",
    "buscados = [f.text.strip().replace('\\n', '') for f in soup7.find_all('h3', class_= 'title')]\n",
    "buscados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url8 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = requests.get(f\"{url8}\").text\n",
    "soup8 = BeautifulSoup(languages, 'html.parser')\n",
    "langlist = soup8.find_all(\"div\", {\"class\": f\"central-featured-lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikilang =[]\n",
    "for l in langlist:\n",
    "    dic_lang={}\n",
    "    dic_lang[\"language\"] = l.find('a').attrs['title'].split()[0]\n",
    "    lis_num = (re.findall(r'\\b\\d+\\b', l.find('bdi').text)) #use regex to get only the digits\n",
    "    dic_lang[\"article_amount\"] = int(''.join(e for e in lis_num))\n",
    "    wikilang.append(dic_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>language</th>\n",
       "      <th>article_amount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>English</td>\n",
       "      <td>6383000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nihongo</td>\n",
       "      <td>1292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Russkiy</td>\n",
       "      <td>1756000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Deutsch</td>\n",
       "      <td>2617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>EspaÃ±ol</td>\n",
       "      <td>1717000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>FranÃ§ais</td>\n",
       "      <td>2362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Italiano</td>\n",
       "      <td>1718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ZhÅngwÃ©n</td>\n",
       "      <td>1231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Polski</td>\n",
       "      <td>1490000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>PortuguÃªs</td>\n",
       "      <td>1074000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     language  article_amount\n",
       "0     English         6383000\n",
       "1     Nihongo         1292000\n",
       "2     Russkiy         1756000\n",
       "3     Deutsch         2617000\n",
       "4    EspaÃ±ol         1717000\n",
       "5   FranÃ§ais         2362000\n",
       "6    Italiano         1718000\n",
       "7  ZhÅngwÃ©n         1231000\n",
       "8      Polski         1490000\n",
       "9  PortuguÃªs         1074000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(wikilang) #not asked for the lab but cute\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 8 - A list with the different kind of datasets available in data.gov.uk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url82 = 'https://data.gov.uk/'\n",
    "dats = requests.get(f\"{url82}\")\n",
    "soup8 = BeautifulSoup(dats.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = soup.find_all('a',{'class':\"govuk-link\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lista = [tag.getText()for tag in datasets]\n",
    "lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url9 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "tenlang = requests.get(url9)\n",
    "soup9 = BeautifulSoup(tenlang.content,\"html.parser\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idiomas = soup.findAll(\"table\")\n",
    "idiomas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stepping up the game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "#url7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "url7 = \"https://www.emsc-csem.org/#2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "ea_html = requests.get(url7).text\n",
    "soup_ea = BeautifulSoup(ea_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           date        time   latitude   longitude  \\\n",
       "0   2022-02-03  13:26:15.2  34.69 S     73.54 W     \n",
       "1   2022-02-03  12:27:07.0   4.08 S     80.95 W     \n",
       "2   2022-02-03  12:03:59.9  39.87 N     21.97 E     \n",
       "3   2022-02-03  12:02:48.0  20.68 S     69.14 W     \n",
       "4   2022-02-03  11:58:36.0  41.94 N     20.10 E     \n",
       "..         ...         ...        ...         ...   \n",
       "70  2022-02-02  22:16:28.0  10.81 S     78.53 W     \n",
       "71  2022-02-02  22:13:43.4  13.99 N     92.35 W     \n",
       "72  2022-02-02  21:26:02.0  16.89 N     99.85 W     \n",
       "73  2022-02-02  21:25:44.0   0.29 N    121.92 E     \n",
       "74  2022-02-02  21:23:49.0  17.00 N     99.88 W     \n",
       "\n",
       "                            region  \n",
       "0    OFF COAST OF O'HIGGINS, CHILE  \n",
       "1       PERU-ECUADOR BORDER REGION  \n",
       "2                           GREECE  \n",
       "3                  TARAPACA, CHILE  \n",
       "4                          ALBANIA  \n",
       "..                             ...  \n",
       "70      NEAR COAST OF CENTRAL PERU  \n",
       "71              OFFSHORE GUATEMALA  \n",
       "72                GUERRERO, MEXICO  \n",
       "73   MINAHASA, SULAWESI, INDONESIA  \n",
       "74                GUERRERO, MEXICO  \n",
       "\n",
       "[75 rows x 5 columns]>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [] #this is going to be a list of lists, each element will be the info of a earthquake\n",
    "table = soup_ea.find_all('tr', class_= 'normal')\n",
    "\n",
    "for td in table:\n",
    "    #print(td)\n",
    "    temp_fecha = td.contents[3].find('a').text\n",
    "    temp_fecha = temp_fecha.split()\n",
    "    fecha = temp_fecha[0]\n",
    "    hora = temp_fecha[1]\n",
    "    \n",
    "    latitud = td.contents[4].text + td.contents[5].text\n",
    "    longitud = td.contents[6].text + td.contents[7].text\n",
    "    region = td.contents[11].text\n",
    "    \n",
    "    data.append([fecha, hora, latitud, longitud, region])\n",
    "\n",
    "terremotos = pd.DataFrame(data, columns = ['date', 'time', 'latitude', 'longitude','region'])\n",
    "terremotos.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>13:26:15.2</td>\n",
       "      <td>34.69 S</td>\n",
       "      <td>73.54 W</td>\n",
       "      <td>OFF COAST OF O'HIGGINS, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>12:27:07.0</td>\n",
       "      <td>4.08 S</td>\n",
       "      <td>80.95 W</td>\n",
       "      <td>PERU-ECUADOR BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>12:03:59.9</td>\n",
       "      <td>39.87 N</td>\n",
       "      <td>21.97 E</td>\n",
       "      <td>GREECE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>12:02:48.0</td>\n",
       "      <td>20.68 S</td>\n",
       "      <td>69.14 W</td>\n",
       "      <td>TARAPACA, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>11:58:36.0</td>\n",
       "      <td>41.94 N</td>\n",
       "      <td>20.10 E</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>11:56:03.0</td>\n",
       "      <td>15.62 N</td>\n",
       "      <td>95.24 W</td>\n",
       "      <td>OFFSHORE OAXACA, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>11:37:43.9</td>\n",
       "      <td>7.45 S</td>\n",
       "      <td>119.67 E</td>\n",
       "      <td>FLORES SEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>11:19:44.0</td>\n",
       "      <td>31.85 S</td>\n",
       "      <td>67.99 W</td>\n",
       "      <td>SAN JUAN, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>10:35:53.0</td>\n",
       "      <td>30.60 S</td>\n",
       "      <td>71.39 W</td>\n",
       "      <td>COQUIMBO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>10:22:46.3</td>\n",
       "      <td>29.82 S</td>\n",
       "      <td>176.88 W</td>\n",
       "      <td>KERMADEC ISLANDS REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>10:21:27.0</td>\n",
       "      <td>0.61 N</td>\n",
       "      <td>98.18 E</td>\n",
       "      <td>NIAS REGION, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>10:09:30.0</td>\n",
       "      <td>9.95 N</td>\n",
       "      <td>84.14 W</td>\n",
       "      <td>COSTA RICA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>10:07:31.0</td>\n",
       "      <td>18.49 N</td>\n",
       "      <td>73.70 W</td>\n",
       "      <td>HAITI REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>10:03:33.6</td>\n",
       "      <td>34.11 N</td>\n",
       "      <td>35.72 E</td>\n",
       "      <td>LEBANON - SYRIA REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>09:55:32.0</td>\n",
       "      <td>31.94 N</td>\n",
       "      <td>117.14 W</td>\n",
       "      <td>OFFSHORE BAJA CALIFORNIA, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>09:41:57.0</td>\n",
       "      <td>30.84 S</td>\n",
       "      <td>69.65 W</td>\n",
       "      <td>SAN JUAN, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>09:35:52.4</td>\n",
       "      <td>23.27 S</td>\n",
       "      <td>64.95 W</td>\n",
       "      <td>SALTA, ARGENTINA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>09:32:15.0</td>\n",
       "      <td>56.33 N</td>\n",
       "      <td>149.93 W</td>\n",
       "      <td>GULF OF ALASKA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>09:31:47.0</td>\n",
       "      <td>15.46 N</td>\n",
       "      <td>93.79 W</td>\n",
       "      <td>OFFSHORE CHIAPAS, MEXICO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>09:25:43.0</td>\n",
       "      <td>31.82 S</td>\n",
       "      <td>71.97 W</td>\n",
       "      <td>OFFSHORE COQUIMBO, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2022-02-03</td>\n",
       "      <td>09:15:06.3</td>\n",
       "      <td>34.12 N</td>\n",
       "      <td>35.70 E</td>\n",
       "      <td>LEBANON - SYRIA REGION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          date        time   latitude   longitude  \\\n",
       "0   2022-02-03  13:26:15.2  34.69 S     73.54 W     \n",
       "1   2022-02-03  12:27:07.0   4.08 S     80.95 W     \n",
       "2   2022-02-03  12:03:59.9  39.87 N     21.97 E     \n",
       "3   2022-02-03  12:02:48.0  20.68 S     69.14 W     \n",
       "4   2022-02-03  11:58:36.0  41.94 N     20.10 E     \n",
       "5   2022-02-03  11:56:03.0  15.62 N     95.24 W     \n",
       "6   2022-02-03  11:37:43.9   7.45 S    119.67 E     \n",
       "7   2022-02-03  11:19:44.0  31.85 S     67.99 W     \n",
       "8   2022-02-03  10:35:53.0  30.60 S     71.39 W     \n",
       "9   2022-02-03  10:22:46.3  29.82 S    176.88 W     \n",
       "10  2022-02-03  10:21:27.0   0.61 N     98.18 E     \n",
       "11  2022-02-03  10:09:30.0   9.95 N     84.14 W     \n",
       "12  2022-02-03  10:07:31.0  18.49 N     73.70 W     \n",
       "13  2022-02-03  10:03:33.6  34.11 N     35.72 E     \n",
       "14  2022-02-03  09:55:32.0  31.94 N    117.14 W     \n",
       "15  2022-02-03  09:41:57.0  30.84 S     69.65 W     \n",
       "16  2022-02-03  09:35:52.4  23.27 S     64.95 W     \n",
       "17  2022-02-03  09:32:15.0  56.33 N    149.93 W     \n",
       "18  2022-02-03  09:31:47.0  15.46 N     93.79 W     \n",
       "19  2022-02-03  09:25:43.0  31.82 S     71.97 W     \n",
       "20  2022-02-03  09:15:06.3  34.12 N     35.70 E     \n",
       "\n",
       "                               region  \n",
       "0       OFF COAST OF O'HIGGINS, CHILE  \n",
       "1          PERU-ECUADOR BORDER REGION  \n",
       "2                              GREECE  \n",
       "3                     TARAPACA, CHILE  \n",
       "4                             ALBANIA  \n",
       "5             OFFSHORE OAXACA, MEXICO  \n",
       "6                          FLORES SEA  \n",
       "7                 SAN JUAN, ARGENTINA  \n",
       "8                     COQUIMBO, CHILE  \n",
       "9             KERMADEC ISLANDS REGION  \n",
       "10             NIAS REGION, INDONESIA  \n",
       "11                         COSTA RICA  \n",
       "12                       HAITI REGION  \n",
       "13             LEBANON - SYRIA REGION  \n",
       "14   OFFSHORE BAJA CALIFORNIA, MEXICO  \n",
       "15                SAN JUAN, ARGENTINA  \n",
       "16                   SALTA, ARGENTINA  \n",
       "17                     GULF OF ALASKA  \n",
       "18           OFFSHORE CHIAPAS, MEXICO  \n",
       "19           OFFSHORE COQUIMBO, CHILE  \n",
       "20             LEBANON - SYRIA REGION  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Once we are sure that the registers are ordered from lastest to oldest we can get the last 20 earthquakes just using the indexes\n",
    "last_terr = terremotos[:21]\n",
    "last_terr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url11 = 'https://www.imdb.com/chart/top'\n",
    "pelis = requests.get(url11)\n",
    "soup = BeautifulSoup(pelis.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = soup.find_all(\"tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [ tags[i].find_all(\"td\")[1].find(\"a\").get(\"href\")  for i in range(1,len(tags))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "250"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Frank Darabont (dir.), Tim Robbins, Morgan Freeman',\n",
       " 'Francis Ford Coppola (dir.), Marlon Brando, Al Pacino',\n",
       " 'Francis Ford Coppola (dir.), Al Pacino, Robert De Niro',\n",
       " 'Christopher Nolan (dir.), Christian Bale, Heath Ledger',\n",
       " 'Sidney Lumet (dir.), Henry Fonda, Lee J. Cobb',\n",
       " 'Steven Spielberg (dir.), Liam Neeson, Ralph Fiennes',\n",
       " 'Peter Jackson (dir.), Elijah Wood, Viggo Mortensen',\n",
       " 'Quentin Tarantino (dir.), John Travolta, Uma Thurman',\n",
       " 'Sergio Leone (dir.), Clint Eastwood, Eli Wallach',\n",
       " 'Peter Jackson (dir.), Elijah Wood, Ian McKellen',\n",
       " 'David Fincher (dir.), Brad Pitt, Edward Norton',\n",
       " 'Robert Zemeckis (dir.), Tom Hanks, Robin Wright',\n",
       " 'Christopher Nolan (dir.), Leonardo DiCaprio, Joseph Gordon-Levitt',\n",
       " 'Peter Jackson (dir.), Elijah Wood, Ian McKellen',\n",
       " 'Irvin Kershner (dir.), Mark Hamill, Harrison Ford',\n",
       " 'Lana Wachowski (dir.), Keanu Reeves, Laurence Fishburne',\n",
       " 'Martin Scorsese (dir.), Robert De Niro, Ray Liotta',\n",
       " 'Milos Forman (dir.), Jack Nicholson, Louise Fletcher',\n",
       " 'Akira Kurosawa (dir.), Toshirô Mifune, Takashi Shimura',\n",
       " 'David Fincher (dir.), Morgan Freeman, Brad Pitt',\n",
       " 'Jonathan Demme (dir.), Jodie Foster, Anthony Hopkins',\n",
       " 'Fernando Meirelles (dir.), Alexandre Rodrigues, Leandro Firmino',\n",
       " 'Roberto Benigni (dir.), Roberto Benigni, Nicoletta Braschi',\n",
       " 'Frank Capra (dir.), James Stewart, Donna Reed',\n",
       " 'Jon Watts (dir.), Tom Holland, Zendaya',\n",
       " 'Steven Spielberg (dir.), Tom Hanks, Matt Damon',\n",
       " 'George Lucas (dir.), Mark Hamill, Harrison Ford',\n",
       " 'Christopher Nolan (dir.), Matthew McConaughey, Anne Hathaway',\n",
       " 'Hayao Miyazaki (dir.), Daveigh Chase, Suzanne Pleshette',\n",
       " 'Frank Darabont (dir.), Tom Hanks, Michael Clarke Duncan']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stars = [tags[i].find_all(\"td\")[1].find(\"a\").get(\"title\")  for i in range(1,len(tags))]\n",
    "stars[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = list(soup.find_all(\"span\", {\"class\": \"secondaryInfo\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1994, 1972, 1974, 2008, 1957, 1993, 2003, 1994, 1966, 2001]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years_bien=[]\n",
    "for a in years:\n",
    "    years_bien.append(int(a.getText().split(\"(\")[1].split(\")\")[0]))\n",
    "years_bien[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Cadena perpetua'],\n",
       " ['El padrino'],\n",
       " ['El padrino: Parte II'],\n",
       " ['El caballero oscuro'],\n",
       " ['12 hombres sin piedad'],\n",
       " ['La lista de Schindler'],\n",
       " ['El señor de los anillos: El retorno del rey'],\n",
       " ['Pulp Fiction'],\n",
       " ['El bueno, el feo y el malo'],\n",
       " ['El señor de los anillos: La comunidad del anillo'],\n",
       " ['El club de la lucha'],\n",
       " ['Forrest Gump'],\n",
       " ['Origen'],\n",
       " ['El señor de los anillos: Las dos torres'],\n",
       " ['El Imperio contraataca'],\n",
       " ['Matrix'],\n",
       " ['Uno de los nuestros'],\n",
       " ['Alguien voló sobre el nido del cuco'],\n",
       " ['Los siete samuráis'],\n",
       " ['Seven'],\n",
       " ['El silencio de los corderos'],\n",
       " ['Ciudad de Dios'],\n",
       " ['La vida es bella'],\n",
       " ['¡Qué bello es vivir!'],\n",
       " ['Spider-Man: No Way Home'],\n",
       " ['Salvar al soldado Ryan'],\n",
       " ['La guerra de las galaxias'],\n",
       " ['Interstellar'],\n",
       " ['El viaje de Chihiro'],\n",
       " ['La milla verde'],\n",
       " ['Parásitos'],\n",
       " ['El profesional (Léon)'],\n",
       " ['Harakiri'],\n",
       " ['El pianista'],\n",
       " ['Terminator 2: El juicio final'],\n",
       " ['Regreso al futuro'],\n",
       " ['Sospechosos habituales'],\n",
       " ['Psicosis'],\n",
       " ['El rey león'],\n",
       " ['Tiempos modernos'],\n",
       " ['La tumba de las luciérnagas'],\n",
       " ['American History X'],\n",
       " ['Whiplash'],\n",
       " ['Gladiator (El gladiador)'],\n",
       " ['Luces de la ciudad'],\n",
       " ['Infiltrados'],\n",
       " ['Intocable'],\n",
       " ['El truco final (El prestigio)'],\n",
       " ['Casablanca'],\n",
       " ['Hasta que llegó su hora'],\n",
       " ['La ventana indiscreta'],\n",
       " ['Cinema Paradiso'],\n",
       " ['Alien, el octavo pasajero'],\n",
       " ['Apocalypse Now'],\n",
       " ['Memento'],\n",
       " ['En busca del arca perdida'],\n",
       " ['El gran dictador'],\n",
       " ['Django desencadenado'],\n",
       " ['La vida de los otros'],\n",
       " ['Senderos de gloria'],\n",
       " ['El crepúsculo de los dioses'],\n",
       " ['WALL·E'],\n",
       " ['Vengadores: Infinity War'],\n",
       " ['Testigo de cargo'],\n",
       " ['Spider-Man: Un nuevo universo'],\n",
       " ['El resplandor'],\n",
       " ['¿Teléfono rojo? Volamos hacia Moscú'],\n",
       " ['La princesa Mononoke'],\n",
       " ['Old Boy'],\n",
       " ['Joker'],\n",
       " ['Your Name.'],\n",
       " ['Coco'],\n",
       " ['El caballero oscuro: La leyenda renace'],\n",
       " ['Aliens: El regreso'],\n",
       " ['Érase una vez en América'],\n",
       " ['Vengadores: Endgame'],\n",
       " ['Cafarnaúm'],\n",
       " ['El submarino (Das Boot)'],\n",
       " ['El infierno del odio'],\n",
       " ['3 Idiots'],\n",
       " ['Toy Story'],\n",
       " ['Amadeus'],\n",
       " ['American Beauty'],\n",
       " ['Braveheart'],\n",
       " ['Malditos bastardos'],\n",
       " ['El indomable Will Hunting'],\n",
       " ['Hamilton'],\n",
       " ['Masacre (Ven y mira)'],\n",
       " ['El retorno del jedi'],\n",
       " ['2001: Una odisea del espacio'],\n",
       " ['Reservoir Dogs'],\n",
       " ['Taare Zameen Par'],\n",
       " ['Vértigo (De entre los muertos)'],\n",
       " ['M, el vampiro de Düsseldorf'],\n",
       " ['La caza'],\n",
       " ['Ciudadano Kane'],\n",
       " ['Réquiem por un sueño'],\n",
       " ['Cantando bajo la lluvia'],\n",
       " ['Con la muerte en los talones'],\n",
       " ['Ikiru (Vivir)'],\n",
       " ['¡Olvídate de mí!'],\n",
       " ['Ladrón de bicicletas'],\n",
       " ['Lawrence de Arabia'],\n",
       " ['El chico'],\n",
       " ['La chaqueta metálica'],\n",
       " ['Incendios'],\n",
       " ['Dangal'],\n",
       " ['El apartamento'],\n",
       " ['Perdición'],\n",
       " ['Metrópolis'],\n",
       " ['La naranja mecánica'],\n",
       " ['Taxi Driver'],\n",
       " ['El padre'],\n",
       " ['El golpe'],\n",
       " ['Nader y Simin, una separación'],\n",
       " ['El precio del poder'],\n",
       " ['Snatch, cerdos y diamantes'],\n",
       " ['1917'],\n",
       " ['Amelie'],\n",
       " ['Matar a un ruiseñor'],\n",
       " ['Toy Story 3'],\n",
       " ['La muerte tenía un precio'],\n",
       " ['Up'],\n",
       " ['Pather Panchali (La canción del camino)'],\n",
       " ['Indiana Jones y la última cruzada'],\n",
       " ['Heat'],\n",
       " ['L.A. Confidential'],\n",
       " ['Ran'],\n",
       " ['Jungla de cristal'],\n",
       " ['Yojimbo'],\n",
       " ['Green Book'],\n",
       " ['Rashomon'],\n",
       " ['El hundimiento'],\n",
       " ['Eva al desnudo'],\n",
       " ['Los caballeros de la mesa cuadrada y sus locos seguidores'],\n",
       " ['Con faldas y a lo loco'],\n",
       " ['Batman Begins'],\n",
       " ['Jai Bhim'],\n",
       " ['Sin perdón'],\n",
       " ['Children of Heaven'],\n",
       " ['El castillo ambulante'],\n",
       " ['El lobo de Wall Street'],\n",
       " ['Vencedores o vencidos'],\n",
       " ['Pozos de ambición'],\n",
       " ['Casino'],\n",
       " ['La gran evasión'],\n",
       " ['El tesoro de Sierra Madre'],\n",
       " ['El laberinto del fauno'],\n",
       " ['Una mente maravillosa'],\n",
       " ['El secreto de sus ojos'],\n",
       " ['Toro salvaje'],\n",
       " ['Chinatown'],\n",
       " ['Mi vecino Totoro'],\n",
       " ['Shutter Island'],\n",
       " ['Lock & Stock'],\n",
       " ['No es país para viejos'],\n",
       " ['Klaus'],\n",
       " ['Crimen perfecto'],\n",
       " ['La cosa'],\n",
       " ['La quimera del oro'],\n",
       " ['Tres anuncios en las afueras'],\n",
       " ['El séptimo sello'],\n",
       " ['El hombre elefante'],\n",
       " ['El sexto sentido'],\n",
       " ['Dersu Uzala (El cazador)'],\n",
       " ['Jurassic Park (Parque Jurásico)'],\n",
       " ['El show de Truman'],\n",
       " ['Fresas salvajes'],\n",
       " ['El tercer hombre'],\n",
       " ['Memories of Murder (Crónica de un asesino en serie)'],\n",
       " ['V de Vendetta'],\n",
       " ['Blade Runner'],\n",
       " ['Trainspotting'],\n",
       " ['Fargo'],\n",
       " ['El puente sobre el río Kwai'],\n",
       " ['Del revés (Inside Out)'],\n",
       " ['Buscando a Nemo'],\n",
       " ['Kill Bill: Volumen 1'],\n",
       " ['Warrior'],\n",
       " ['Lo que el viento se llevó'],\n",
       " ['Cuentos de Tokio'],\n",
       " ['La ley del silencio'],\n",
       " ['Mi padre y mi hijo'],\n",
       " ['Prisioneros'],\n",
       " ['Relatos salvajes'],\n",
       " ['Stalker'],\n",
       " ['El gran hotel Budapest'],\n",
       " ['El cazador'],\n",
       " ['El maquinista de La General'],\n",
       " ['Persona'],\n",
       " ['El moderno Sherlock Holmes'],\n",
       " ['Gran Torino'],\n",
       " ['Antes de amanecer'],\n",
       " ['Mary and Max'],\n",
       " ['Atrápame si puedes'],\n",
       " ['Caballero sin espada'],\n",
       " ['Barry Lyndon'],\n",
       " ['En el nombre del padre'],\n",
       " ['Dune'],\n",
       " ['Hasta el último hombre'],\n",
       " ['Perdida'],\n",
       " ['Z'],\n",
       " ['La habitación'],\n",
       " ['Andhadhun'],\n",
       " ['La pasión de Juana de Arco'],\n",
       " [\"Le Mans '66\"],\n",
       " ['12 años de esclavitud'],\n",
       " ['Ser o no ser'],\n",
       " ['El gran Lebowski'],\n",
       " ['El club de los poetas muertos'],\n",
       " ['Harry Potter y las Reliquias de la Muerte - Parte 2'],\n",
       " ['Ben-Hur'],\n",
       " ['Cómo entrenar a tu dragón'],\n",
       " ['Mad Max: Furia en la carretera'],\n",
       " ['Sonata de otoño'],\n",
       " ['Million Dollar Baby'],\n",
       " ['El salario del miedo'],\n",
       " ['Cuenta conmigo'],\n",
       " ['Network, un mundo implacable'],\n",
       " ['La doncella'],\n",
       " ['Logan'],\n",
       " ['El odio'],\n",
       " ['Gangs of Wasseypur'],\n",
       " ['La leyenda del indomable'],\n",
       " ['A Silent Voice'],\n",
       " ['Siempre a tu lado (Hachiko)'],\n",
       " ['Los cuatrocientos golpes'],\n",
       " ['Platoon'],\n",
       " ['Spotlight'],\n",
       " ['Monstruos, S.A.'],\n",
       " ['Rebeca'],\n",
       " ['La vida de Brian'],\n",
       " ['Deseando amar'],\n",
       " ['Eskiya'],\n",
       " ['Hotel Rwanda'],\n",
       " ['Rush'],\n",
       " ['Rocky'],\n",
       " ['Amores perros'],\n",
       " ['Hacia rutas salvajes'],\n",
       " ['Nausicaä del Valle del Viento'],\n",
       " ['Drishyam'],\n",
       " ['Antes del atardecer'],\n",
       " ['Sucedió una noche'],\n",
       " ['Guardianes de la noche - Kimetsu no Yaiba - La película: Tren Infinito'],\n",
       " ['Fanny y Alexander'],\n",
       " ['Hera Pheri'],\n",
       " ['Drishyam 2'],\n",
       " ['La batalla de Argel'],\n",
       " ['Las noches de Cabiria'],\n",
       " ['Andrei Rublev']]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titulos = [list(tags[i].find_all(\"td\")[1].find(\"a\"))  for i in range(1,len(tags))]\n",
    "titulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=[[titulos[i][0], years_bien[i], stars[i].split(\"(dir.)\")[0], stars[i].split(\"(dir.)\")[1].lstrip(\",\") ] for  i in range(1,len(titulos)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titulo</th>\n",
       "      <th>año</th>\n",
       "      <th>director</th>\n",
       "      <th>elenco</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>El padrino</td>\n",
       "      <td>1972</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Marlon Brando, Al Pacino</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>El padrino: Parte II</td>\n",
       "      <td>1974</td>\n",
       "      <td>Francis Ford Coppola</td>\n",
       "      <td>Al Pacino, Robert De Niro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>El caballero oscuro</td>\n",
       "      <td>2008</td>\n",
       "      <td>Christopher Nolan</td>\n",
       "      <td>Christian Bale, Heath Ledger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12 hombres sin piedad</td>\n",
       "      <td>1957</td>\n",
       "      <td>Sidney Lumet</td>\n",
       "      <td>Henry Fonda, Lee J. Cobb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>La lista de Schindler</td>\n",
       "      <td>1993</td>\n",
       "      <td>Steven Spielberg</td>\n",
       "      <td>Liam Neeson, Ralph Fiennes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244</th>\n",
       "      <td>Hera Pheri</td>\n",
       "      <td>2000</td>\n",
       "      <td>Priyadarshan</td>\n",
       "      <td>Akshay Kumar, Suniel Shetty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>Drishyam 2</td>\n",
       "      <td>2021</td>\n",
       "      <td>Jeethu Joseph</td>\n",
       "      <td>Mohanlal, Meena</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>La batalla de Argel</td>\n",
       "      <td>1966</td>\n",
       "      <td>Gillo Pontecorvo</td>\n",
       "      <td>Brahim Hadjadj, Jean Martin</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>Las noches de Cabiria</td>\n",
       "      <td>1957</td>\n",
       "      <td>Federico Fellini</td>\n",
       "      <td>Giulietta Masina, François Périer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>Andrei Rublev</td>\n",
       "      <td>1966</td>\n",
       "      <td>Andrei Tarkovsky</td>\n",
       "      <td>Anatoliy Solonitsyn, Ivan Lapikov</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>249 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    titulo   año               director  \\\n",
       "0               El padrino  1972  Francis Ford Coppola    \n",
       "1     El padrino: Parte II  1974  Francis Ford Coppola    \n",
       "2      El caballero oscuro  2008     Christopher Nolan    \n",
       "3    12 hombres sin piedad  1957          Sidney Lumet    \n",
       "4    La lista de Schindler  1993      Steven Spielberg    \n",
       "..                     ...   ...                    ...   \n",
       "244             Hera Pheri  2000          Priyadarshan    \n",
       "245             Drishyam 2  2021         Jeethu Joseph    \n",
       "246    La batalla de Argel  1966      Gillo Pontecorvo    \n",
       "247  Las noches de Cabiria  1957      Federico Fellini    \n",
       "248          Andrei Rublev  1966      Andrei Tarkovsky    \n",
       "\n",
       "                                 elenco  \n",
       "0              Marlon Brando, Al Pacino  \n",
       "1             Al Pacino, Robert De Niro  \n",
       "2          Christian Bale, Heath Ledger  \n",
       "3              Henry Fonda, Lee J. Cobb  \n",
       "4            Liam Neeson, Ralph Fiennes  \n",
       "..                                  ...  \n",
       "244         Akshay Kumar, Suniel Shetty  \n",
       "245                     Mohanlal, Meena  \n",
       "246         Brahim Hadjadj, Jean Martin  \n",
       "247   Giulietta Masina, François Périer  \n",
       "248   Anatoliy Solonitsyn, Ivan Lapikov  \n",
       "\n",
       "[249 rows x 4 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.DataFrame(final, columns= [\"titulo\", \"año\", \"director\",\"elenco\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather(city):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Challenge 14 - Book name,price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url14 = 'http://books.toscrape.com/'\n",
    "books = requests.get(url11)\n",
    "soup = BeautifulSoup(books.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "libros = list(soup.select('h1.h3 a[tittle]'))\n",
    "#nombres = soup.select('h1.h3 a[tittle]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "libros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "libros[1]\n",
    "for i in libros:\n",
    "    print(i.text)\n",
    "    break\n",
    "#enlaces = [tag.get('href') for tag in tags]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Did you limit your output? Thank you! 🙂**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
